{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5895bc53-ca8c-4cbd-aff8-c62f5d2366b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import keras_hub\n",
    "import os, shutil, pathlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc220ba-429d-441f-83c0-287ef6156db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "images_path =\"coco_dataset/datasets/coco/\"\n",
    "annotations_path=\"coco_dataset/datasets/annotations/annotations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d9df340-7126-4189-8ee1-b6189a984d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{annotations_path}instances_train2017.json\", \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "images = {image[\"id\"]: image for image in annotations[\"images\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b47511d-5bec-4853-930f-856ff51a60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_box(box, width, height):\n",
    "    scale = 1.0 / max(width, height)\n",
    "    x, y, w, h = [v * scale for v in box]\n",
    "    x += (height - width) * scale / 2 if height > width else 0\n",
    "    y += (width - height) * scale / 2 if width > height else 0\n",
    "    return [x, y, w, h]\n",
    "\n",
    "metadata = {}\n",
    "for annotation in annotations[\"annotations\"]:\n",
    "    id = annotation[\"image_id\"]\n",
    "    if id not in metadata:\n",
    "        metadata[id] = {\"boxes\": [], \"labels\": []}\n",
    "    image = images[id]\n",
    "    box = scale_box(annotation[\"bbox\"], image[\"width\"], image[\"height\"])\n",
    "    metadata[id][\"boxes\"].append(box)\n",
    "    metadata[id][\"labels\"].append(annotation[\"category_id\"])\n",
    "    metadata[id][\"path\"] = images_path + \"train2017/\" + image[\"file_name\"]\n",
    "metadata = list(metadata.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229849b9-b187-4111-9a98-336d5cf77201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking count of unique values of label more than 4 for test and <= for train and val\n",
    "import random\n",
    "from collections import Counter\n",
    "metadata_1 = [x for x in metadata if len(set(tuple(box) for box in x[\"boxes\"])) <= 4]\n",
    "random.shuffle(metadata_1)\n",
    "metadata_2 = [x for x in metadata if len(set(tuple(box) for box in x[\"boxes\"])) > 4]\n",
    "random.shuffle(metadata_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "184bb773-b642-4c65-9a60-22d0444d388b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': [[0.06839999999999999, 0.06278, 0.852, 0.8923800000000001]],\n",
       " 'labels': [86],\n",
       " 'path': 'coco_dataset/datasets/coco/train2017/000000279909.jpg'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_1[433]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43518485-a2c5-45b4-a0a2-bdbca2fffec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class map example: 1 -> 0, 62 -> 56\n",
      "Created /work/Notebooks/yolo_dataset/images/train and /work/Notebooks/yolo_dataset/labels/train\n",
      "Created /work/Notebooks/yolo_dataset/images/val and /work/Notebooks/yolo_dataset/labels/val\n",
      "Symlinked 000000014781.jpg -> /work/Notebooks/coco_dataset/datasets/coco/train2017/000000014781.jpg\n",
      "Symlinked 000000127997.jpg -> /work/Notebooks/coco_dataset/datasets/coco/train2017/000000127997.jpg\n",
      "Symlinked 000000533941.jpg -> /work/Notebooks/coco_dataset/datasets/coco/train2017/000000533941.jpg\n",
      "Symlinked 000000282473.jpg -> /work/Notebooks/coco_dataset/datasets/coco/train2017/000000282473.jpg\n",
      "Symlinked 000000186034.jpg -> /work/Notebooks/coco_dataset/datasets/coco/train2017/000000186034.jpg\n",
      "train: 47364 files, 0 link fails, 0 invalid annos\n",
      "Symlinked 000000312662.jpg -> /work/Notebooks/coco_dataset/datasets/coco/train2017/000000312662.jpg\n",
      "Symlinked 000000564339.jpg -> /work/Notebooks/coco_dataset/datasets/coco/train2017/000000564339.jpg\n",
      "Symlinked 000000435988.jpg -> /work/Notebooks/coco_dataset/datasets/coco/train2017/000000435988.jpg\n",
      "Symlinked 000000436161.jpg -> /work/Notebooks/coco_dataset/datasets/coco/train2017/000000436161.jpg\n",
      "Symlinked 000000388744.jpg -> /work/Notebooks/coco_dataset/datasets/coco/train2017/000000388744.jpg\n",
      "val: 11841 files, 0 link fails, 0 invalid annos\n",
      "Final check: 47364 symlinks in train/ (expected ~47364)\n",
      "Sample label (000000525265.txt): ['61 0.772008 0.887703 0.205984 0.200906\\n', '71 0.342359 0.664313 0.234844 0.090094\\n']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mapping from your annotations (unchanged)\n",
    "coco_categories = annotations[\"categories\"]\n",
    "coco_id_to_index = {cat['id']: idx for idx, cat in enumerate(coco_categories)}\n",
    "print(f\"Class map example: 1 -> {coco_id_to_index.get(1, 'N/A')}, 62 -> {coco_id_to_index.get(62, 'N/A')}\")\n",
    "\n",
    "# Create dirs (absolute for safety)\n",
    "dataset_dir = Path(os.path.abspath(\"./yolo_dataset\"))  # Full path\n",
    "for split in ['train', 'val']:\n",
    "    img_split = dataset_dir / 'images' / split\n",
    "    lbl_split = dataset_dir / 'labels' / split\n",
    "    img_split.mkdir(parents=True, exist_ok=True)\n",
    "    lbl_split.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created {img_split} and {lbl_split}\")\n",
    "\n",
    "# Split metadata_1\n",
    "val_size = int(0.2 * len(metadata_1))\n",
    "train_metadata = metadata_1[:-val_size]\n",
    "val_metadata = metadata_1[-val_size:]\n",
    "\n",
    "def convert_split(metadata_split, split_name):\n",
    "    count = 0\n",
    "    link_fail = 0\n",
    "    invalid_count = 0\n",
    "    for sample in metadata_split:\n",
    "        img_name = Path(sample[\"path\"]).name\n",
    "        src_img = os.path.abspath(sample[\"path\"])  # Absolute source for symlink\n",
    "        \n",
    "        # Symlink image (no copy!)\n",
    "        dst_img = dataset_dir / 'images' / split_name / img_name\n",
    "        if not dst_img.exists():\n",
    "            try:\n",
    "                if os.path.exists(src_img):\n",
    "                    os.symlink(src_img, dst_img)  # Lightweight link\n",
    "                    print(f\"Symlinked {img_name} -> {src_img}\") if count < 5 else None  # Debug first 5\n",
    "                else:\n",
    "                    print(f\"SKIP: Source {src_img} not found for {img_name}\")\n",
    "                    link_fail += 1\n",
    "                    continue\n",
    "            except OSError as e:\n",
    "                print(f\"LINK FAIL for {img_name}: {str(e)} (e.g., cross-device if volumes differ)\")\n",
    "                link_fail += 1\n",
    "                continue\n",
    "        \n",
    "        # Create label (unchanged; writes to disk, but tiny)\n",
    "        dst_lbl = dataset_dir / 'labels' / split_name / img_name.replace('.jpg', '.txt')\n",
    "        with open(dst_lbl, 'w') as f:\n",
    "            for box, label in zip(sample[\"boxes\"], sample[\"labels\"]):\n",
    "                yolo_cls = coco_id_to_index.get(label, -1)\n",
    "                if yolo_cls == -1:\n",
    "                    print(f\"WARNING: Invalid class {label} in {img_name}\")\n",
    "                    invalid_count += 1\n",
    "                    continue\n",
    "                x_center = max(0.0, min(1.0, box[0] + box[2] / 2))\n",
    "                y_center = max(0.0, min(1.0, box[1] + box[3] / 2))\n",
    "                w = max(0.0, min(1.0, box[2]))\n",
    "                h = max(0.0, min(1.0, box[3]))\n",
    "                if w > 0 and h > 0:\n",
    "                    f.write(f\"{yolo_cls} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "                else:\n",
    "                    invalid_count += 1\n",
    "        \n",
    "        count += 1\n",
    "        #if count % 100 == 0:\n",
    "         #   print(f\"Processed {count} in {split_name} (link fails: {link_fail})\")\n",
    "    \n",
    "    print(f\"{split_name}: {count} files, {link_fail} link fails, {invalid_count} invalid annos\")\n",
    "    return count\n",
    "\n",
    "train_count = convert_split(train_metadata, 'train')\n",
    "val_count = convert_split(val_metadata, 'val')\n",
    "\n",
    "# Validation (check symlinks)\n",
    "train_img_dir = dataset_dir / 'images' / 'train'\n",
    "num_links = len(list(train_img_dir.glob('*.jpg')))\n",
    "print(f\"Final check: {num_links} symlinks in train/ (expected ~{len(train_metadata)})\")\n",
    "if num_links > 0:\n",
    "    # Test a symlink (should resolve to original)\n",
    "    sample_link = list(train_img_dir.glob('*.jpg'))[0]\n",
    "    #print(f\"Sample symlink {sample_link.name} resolves to: {os.path.realpath(sample_link)}\")\n",
    "    # Sample label as before\n",
    "    sample_lbl = list((dataset_dir / 'labels' / 'train').glob('*.txt'))[0]\n",
    "    with open(sample_lbl, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"Sample label ({sample_lbl.name}): {lines[:3]}\")\n",
    "else:\n",
    "    print(\"STILL EMPTY! Check link fails above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3138abf4-5a70-4706-a701-bb15aec07369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 80 classes: ['person', 'bicycle', 'car', 'motorcycle', 'airplane']...['teddy bear', 'hair drier', 'toothbrush']\n",
      "dataset.yaml updated. nc= 80\n"
     ]
    }
   ],
   "source": [
    "# Get all 91 names from your annotations (in order)\n",
    "coco_names = [cat['name'] for cat in sorted(annotations[\"categories\"], key=lambda x: x['id'])]\n",
    "print(f\"Using {len(coco_names)} classes: {coco_names[:5]}...{coco_names[-3:]}\")\n",
    "\n",
    "yaml_content = f\"\"\"path: ./yolo_dataset  # Relative to notebook\n",
    "train: images/train\n",
    "val: images/val\n",
    "nc: {len(coco_names)}  # 91 from your annotations\n",
    "names: {coco_names}\n",
    "\"\"\"\n",
    "with open(\"coco_custom.yaml\", 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "print(\"dataset.yaml updated. nc=\", len(coco_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72d26ab4-5b95-4acb-80e4-7edd90aef959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/home/ucloud/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Deleted 0 old cache files\n",
      "Ultralytics 8.3.225 üöÄ Python-3.12.11 torch-2.9.0+cu128 CUDA:0 (NVIDIA L4, 22574MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco_custom.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=4, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8_custom_coco_fixed6, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/work/Notebooks/runs/detect/yolov8_custom_coco_fixed6, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/home/ucloud/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 21.4MB/s 0.0s\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,157,200 parameters, 3,157,184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 8.2¬±3.8 MB/s, size: 154.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /work/Notebooks/yolo_dataset/labels/train... 47364 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 47364/47364 324.4it/s 2:260.0sss\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/work/Notebooks/yolo_dataset/images/train/000000522365.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /work/Notebooks/yolo_dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 515.7¬±698.6 MB/s, size: 162.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /work/Notebooks/yolo_dataset/labels/val... 11841 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11841/11841 310.4it/s 38.1s.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /work/Notebooks/yolo_dataset/labels/val.cache\n",
      "Plotting labels to /work/Notebooks/runs/detect/yolov8_custom_coco_fixed6/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/work/Notebooks/runs/detect/yolov8_custom_coco_fixed6\u001b[0m\n",
      "Starting training for 4 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/4      2.24G      1.879      1.785      1.757         15        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2961/2961 8.9it/s 5:31<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 371/371 7.5it/s 49.6s0.3ss\n",
      "                   all      11841      27778       0.61      0.451      0.484      0.259\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/4      5.33G       1.62      1.754      1.538         32        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2961/2961 9.3it/s 5:19<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 371/371 8.5it/s 43.7ss<0.1s\n",
      "                   all      11841      27778      0.596      0.446      0.479      0.274\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        3/4      5.34G      1.547      1.706      1.493         16        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2961/2961 9.7it/s 5:05<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 371/371 9.2it/s 40.1ss<0.1s\n",
      "                   all      11841      27778      0.626      0.452      0.496      0.293\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        4/4      5.36G      1.496      1.652       1.46         22        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2961/2961 9.6it/s 5:099<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 371/371 9.1it/s 40.6s<0.1ss\n",
      "                   all      11841      27778      0.637      0.471      0.521      0.315\n",
      "\n",
      "4 epochs completed in 0.402 hours.\n",
      "Optimizer stripped from /work/Notebooks/runs/detect/yolov8_custom_coco_fixed6/weights/last.pt, 6.5MB\n",
      "Optimizer stripped from /work/Notebooks/runs/detect/yolov8_custom_coco_fixed6/weights/best.pt, 6.5MB\n",
      "\n",
      "Validating /work/Notebooks/runs/detect/yolov8_custom_coco_fixed6/weights/best.pt...\n",
      "Ultralytics 8.3.225 üöÄ Python-3.12.11 torch-2.9.0+cu128 CUDA:0 (NVIDIA L4, 22574MiB)\n",
      "Model summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 371/371 9.4it/s 39.4ss<0.1s\n",
      "                   all      11841      27778      0.637      0.471      0.521      0.315\n",
      "                person       4765       6152      0.807      0.737      0.794      0.518\n",
      "               bicycle        122        139      0.706      0.396      0.463      0.245\n",
      "                   car        500        672      0.608      0.213      0.297      0.159\n",
      "            motorcycle        246        277      0.829      0.768      0.838      0.517\n",
      "              airplane        416        519      0.815       0.74      0.796      0.498\n",
      "                   bus        169        209      0.811      0.732      0.807      0.584\n",
      "                 train        449        532      0.839      0.795      0.841      0.558\n",
      "                 truck        319        386      0.664      0.503       0.57      0.354\n",
      "                  boat        226        326      0.589      0.383       0.42      0.225\n",
      "         traffic light        184        311      0.593      0.289      0.335      0.165\n",
      "          fire hydrant        189        196      0.714       0.77      0.818       0.54\n",
      "             stop sign        190        200      0.719       0.74      0.755      0.527\n",
      "         parking meter         62         85      0.601      0.635      0.631      0.393\n",
      "                 bench        351        408       0.61      0.473      0.509      0.297\n",
      "                  bird        361        510      0.719      0.567      0.614      0.362\n",
      "                   cat        613        684      0.761      0.816      0.859      0.559\n",
      "                   dog        509        576      0.775      0.688      0.762      0.496\n",
      "                 horse        295        437      0.785      0.783      0.839      0.547\n",
      "                 sheep        125        257      0.735      0.716      0.745      0.453\n",
      "                   cow        173        323      0.753      0.666       0.73      0.478\n",
      "              elephant        269        454      0.803      0.839      0.877      0.624\n",
      "                  bear        183        233      0.798      0.824      0.867      0.615\n",
      "                 zebra        288        554      0.822      0.875      0.908      0.636\n",
      "               giraffe        405        707      0.841      0.846      0.893      0.616\n",
      "              backpack        180        185      0.475     0.0734      0.119     0.0366\n",
      "              umbrella        176        202      0.637      0.574      0.608      0.359\n",
      "               handbag        159        169      0.468     0.0414       0.09     0.0369\n",
      "                   tie        263        273      0.504      0.451      0.451      0.195\n",
      "              suitcase        152        201       0.51      0.552      0.537      0.314\n",
      "               frisbee        251        261      0.533      0.284      0.371      0.193\n",
      "                  skis        292        338      0.615      0.175      0.272      0.114\n",
      "             snowboard        178        196        0.6      0.352      0.417      0.222\n",
      "           sports ball        284        293      0.304     0.0372     0.0688     0.0274\n",
      "                  kite        167        192      0.477      0.365      0.364      0.199\n",
      "          baseball bat         94         99      0.404      0.202      0.226     0.0862\n",
      "        baseball glove        118        131      0.543        0.2      0.343      0.138\n",
      "            skateboard        362        385      0.606      0.486      0.539      0.255\n",
      "             surfboard        441        488      0.584      0.432      0.484      0.231\n",
      "         tennis racket        346        370      0.686      0.419      0.531      0.234\n",
      "                bottle        256        300      0.501       0.14      0.197     0.0889\n",
      "            wine glass         40         42      0.651      0.405      0.521      0.321\n",
      "                   cup        233        263      0.619      0.369      0.403      0.249\n",
      "                  fork        117        120      0.589      0.441      0.474      0.259\n",
      "                 knife        112        121      0.448      0.182      0.243      0.137\n",
      "                 spoon         76         81      0.509      0.173      0.226      0.108\n",
      "                  bowl        191        214      0.483      0.383      0.376       0.22\n",
      "                banana        153        194      0.575      0.474      0.528      0.305\n",
      "                 apple         69        105      0.678      0.257      0.341      0.212\n",
      "              sandwich        152        186      0.702      0.543       0.67      0.413\n",
      "                orange         66         86       0.44      0.651      0.593      0.389\n",
      "              broccoli        108        171      0.584      0.433      0.477      0.277\n",
      "                carrot         52         79      0.422      0.296      0.279      0.146\n",
      "               hot dog        101        124      0.785      0.645       0.73      0.508\n",
      "                 pizza        243        287      0.774      0.787      0.831      0.587\n",
      "                 donut         92        130      0.725      0.623      0.696      0.463\n",
      "                  cake        163        189      0.734      0.568      0.668      0.446\n",
      "                 chair        331        379      0.532      0.206      0.256      0.123\n",
      "                 couch        198        217      0.519      0.456      0.465       0.28\n",
      "          potted plant        212        263      0.585      0.252      0.305       0.16\n",
      "                   bed        464        501      0.656      0.681      0.722      0.436\n",
      "          dining table        392        398      0.501       0.42      0.399      0.252\n",
      "                toilet        571        637      0.643      0.708      0.718      0.432\n",
      "                    tv        200        212      0.688       0.58      0.626      0.374\n",
      "                laptop        190        200      0.606      0.737      0.724      0.487\n",
      "                 mouse        104        108      0.478       0.39      0.406      0.226\n",
      "                remote        175        218      0.759      0.217      0.305      0.189\n",
      "              keyboard        141        150      0.623       0.56      0.586      0.336\n",
      "            cell phone        339        369      0.738       0.32      0.417      0.231\n",
      "             microwave         89         94      0.546      0.574      0.538      0.272\n",
      "                  oven        171        181      0.565      0.586      0.564      0.327\n",
      "               toaster          3          3          1          0      0.339      0.304\n",
      "                  sink        428        480      0.519      0.325      0.363      0.173\n",
      "          refrigerator        143        148      0.702      0.605      0.653      0.388\n",
      "                  book        151        194      0.315      0.133      0.157     0.0856\n",
      "                 clock        426        543       0.74      0.433       0.51      0.269\n",
      "                  vase        244        292      0.658      0.568      0.602      0.348\n",
      "              scissors         92        103      0.547      0.369      0.423      0.245\n",
      "            teddy bear        250        321      0.721      0.636      0.719      0.457\n",
      "            hair drier         20         21          1          0     0.0527     0.0229\n",
      "            toothbrush        106        124      0.473      0.137      0.189     0.0839\n",
      "Speed: 0.1ms preprocess, 0.7ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/work/Notebooks/runs/detect/yolov8_custom_coco_fixed6\u001b[0m\n",
      "Training complete! Check runs/detect/yolov8_custom_coco_fixed/ for plots.\n"
     ]
    }
   ],
   "source": [
    "import torch  # For device check\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "\n",
    "# Clear any old cache\n",
    "dataset_dir = Path(\"./yolo_dataset\")\n",
    "cache_files = list(dataset_dir.glob('**/*.cache'))\n",
    "for cache in cache_files:\n",
    "    cache.unlink()\n",
    "print(f\"Deleted {len(cache_files)} old cache files\")\n",
    "\n",
    "# Train\n",
    "model = YOLO('yolov8n.pt')  # Nano for speed\n",
    "results = model.train(\n",
    "    data=\"coco_custom.yaml\",\n",
    "    epochs=4,  # Bump to 50+ for better results later\n",
    "    imgsz=640,\n",
    "    batch=16,  # Lower to 8 if OOM\n",
    "    workers=4,\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    name=\"yolov8_custom_coco_fixed\"\n",
    ")\n",
    "model.save(\"yolov8_retrained.pt\")\n",
    "print(\"Training complete! Check runs/detect/yolov8_custom_coco_fixed/ for plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe0b738e-c09e-4e35-b821-5ccf3e722f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found at index 39473: {'boxes': [[0.091234375, 0.569546875, 0.9018750000000001, 0.23781249999999998], [0.7050468750000001, 0.5797656250000001, 0.06853125, 0.1213125], [0.3253125, 0.61565625, 0.19846875, 0.21637499999999998], [0.7103281250000001, 0.5082031250000001, 0.27159375, 0.323828125], [0.002984375, 0.24706250000000002, 0.12534375, 0.57303125], [0.37868750000000007, 0.4344375, 0.068734375, 0.08904687500000001], [0.42978125, 0.290328125, 0.285015625, 0.5178281250000001], [0.728234375, 0.33809374999999997, 0.076109375, 0.19846875], [0.8356718750000001, 0.36234375, 0.14028125, 0.34023437500000003], [0.7100312500000001, 0.36928125, 0.048906250000000005, 0.09412500000000001], [0.6675312500000001, 0.40890625, 0.065140625, 0.12565625], [0.16718750000000002, 0.271875, 0.29765625, 0.56015625], [0.114546875, 0.43245312500000005, 0.079015625, 0.233203125], [0.8232187500000001, 0.35121875, 0.066859375, 0.17895312500000002], [0.9603125000000001, 0.39001562500000003, 0.0396875, 0.200984375], [0.549125, 0.39221875000000006, 0.042625, 0.09531250000000001], [0.36340625000000004, 0.47053125, 0.03521875, 0.025421875], [0.728515625, 0.503296875, 0.020921875000000003, 0.049375], [0.7534531250000001, 0.50084375, 0.017328125, 0.040843750000000005], [0.11690624999999999, 0.67425, 0.055640625, 0.091421875], [0.571875, 0.74609375, 0.259375, 0.084375]], 'labels': [62, 62, 88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 32, 44, 44, 44, 62, 1], 'path': 'coco_dataset/datasets/coco/train2017/000000019489.jpg'}\n"
     ]
    }
   ],
   "source": [
    "target_path = 'coco_dataset/datasets/coco/train2017/000000019489.jpg'\n",
    "\n",
    "# Find the index\n",
    "for i, item in enumerate(metadata_2):\n",
    "    if item['path'] == target_path:\n",
    "        found_index = i\n",
    "        print(f\"Found at index {i}: {item}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e673ab87-5d7e-4a16-aa06-aaceb397811b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': [[0.091234375, 0.569546875, 0.9018750000000001, 0.23781249999999998],\n",
       "  [0.7050468750000001, 0.5797656250000001, 0.06853125, 0.1213125],\n",
       "  [0.3253125, 0.61565625, 0.19846875, 0.21637499999999998],\n",
       "  [0.7103281250000001, 0.5082031250000001, 0.27159375, 0.323828125],\n",
       "  [0.002984375, 0.24706250000000002, 0.12534375, 0.57303125],\n",
       "  [0.37868750000000007, 0.4344375, 0.068734375, 0.08904687500000001],\n",
       "  [0.42978125, 0.290328125, 0.285015625, 0.5178281250000001],\n",
       "  [0.728234375, 0.33809374999999997, 0.076109375, 0.19846875],\n",
       "  [0.8356718750000001, 0.36234375, 0.14028125, 0.34023437500000003],\n",
       "  [0.7100312500000001, 0.36928125, 0.048906250000000005, 0.09412500000000001],\n",
       "  [0.6675312500000001, 0.40890625, 0.065140625, 0.12565625],\n",
       "  [0.16718750000000002, 0.271875, 0.29765625, 0.56015625],\n",
       "  [0.114546875, 0.43245312500000005, 0.079015625, 0.233203125],\n",
       "  [0.8232187500000001, 0.35121875, 0.066859375, 0.17895312500000002],\n",
       "  [0.9603125000000001, 0.39001562500000003, 0.0396875, 0.200984375],\n",
       "  [0.549125, 0.39221875000000006, 0.042625, 0.09531250000000001],\n",
       "  [0.36340625000000004, 0.47053125, 0.03521875, 0.025421875],\n",
       "  [0.728515625, 0.503296875, 0.020921875000000003, 0.049375],\n",
       "  [0.7534531250000001, 0.50084375, 0.017328125, 0.040843750000000005],\n",
       "  [0.11690624999999999, 0.67425, 0.055640625, 0.091421875],\n",
       "  [0.571875, 0.74609375, 0.259375, 0.084375]],\n",
       " 'labels': [62,\n",
       "  62,\n",
       "  88,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  32,\n",
       "  44,\n",
       "  44,\n",
       "  44,\n",
       "  62,\n",
       "  1],\n",
       " 'path': 'coco_dataset/datasets/coco/train2017/000000019489.jpg'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_2[found_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caa002c8-6f87-4e53-8873-6cf8a36cf21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /work/Notebooks/coco_dataset/datasets/coco/train2017/000000019489.jpg: 448x640 7 persons, 1 teddy bear, 79.2ms\n",
      "Speed: 1.7ms preprocess, 79.2ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Retrained YOLOv8: 8 detections (conf >=0.25)\n",
      "Pred classes: ['person', 'person', 'person', 'person', 'person', 'teddy bear', 'person', 'person']\n",
      "GT classes (unique): {'teddy_bear', 'tie', 'bottle', 'chair', 'person'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from keras_hub.utils import coco_id_to_name  # For GT names\n",
    "\n",
    "# Load retrained model\n",
    "yolov8_retrained = YOLO(\"yolov8_retrained.pt\")\n",
    "\n",
    "# Predict on sample\n",
    "sample = metadata_2[found_index]\n",
    "results = yolov8_retrained(sample[\"path\"], conf=0.25, verbose=True)  # Standard conf\n",
    "\n",
    "# Extract detections\n",
    "detections = []\n",
    "for r in results:\n",
    "    if r.boxes is not None:\n",
    "        for box in r.boxes:\n",
    "            if box.conf[0] >= 0.25:\n",
    "                detections.append({\n",
    "                    'class_id': int(box.cls[0]),\n",
    "                    'class_name': yolov8_retrained.names[int(box.cls[0])],\n",
    "                    'conf': float(box.conf[0]),\n",
    "                    'box': box.xywhn[0].tolist()  # [x_center, y_center, w, h]\n",
    "                })\n",
    "\n",
    "print(f\"Retrained YOLOv8: {len(detections)} detections (conf >=0.25)\")\n",
    "print(f\"Pred classes: {[d['class_name'] for d in detections]}\")\n",
    "\n",
    "# GT for comparison\n",
    "gt_classes = [coco_id_to_name(l) for l in sample[\"labels\"]]\n",
    "print(f\"GT classes (unique): {set(gt_classes)}\")\n",
    "\n",
    "# Visualize YOLOv8 (simple plot)\n",
    "def draw_yolov8_prediction(image_path, detections, cutoff=0.25):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8), dpi=150)\n",
    "    img = plt.imread(image_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(\"YOLOv8 Retrained Predictions\")\n",
    "    \n",
    "    filtered_dets = [d for d in detections if d['conf'] >= cutoff]\n",
    "    for d in filtered_dets:\n",
    "        x, y, w, h = d['box']\n",
    "        rect = Rectangle((x - w/2, y - h/2), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, y - h/2 - 0.01, f\"{d['class_name']} {d['conf']:.2f}\", \n",
    "                color='red', fontsize=8, ha='center', va='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "draw_yolov8_prediction(sample[\"path\"], detections, cutoff=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416ec76-a513-4901-b6a7-02914c2fd68c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
